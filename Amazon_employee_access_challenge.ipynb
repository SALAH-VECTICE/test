{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SALAH-VECTICE/SDK_notebooks/blob/main/Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpmSSdxOTISb",
    "papermill": {
     "duration": 0.045127,
     "end_time": "2020-10-15T16:08:16.602733",
     "exception": false,
     "start_time": "2020-10-15T16:08:16.557606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Amazon.com - Employee Access Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGo38hdyTISe",
    "papermill": {
     "duration": 0.043196,
     "end_time": "2020-10-15T16:08:16.691894",
     "exception": false,
     "start_time": "2020-10-15T16:08:16.648698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Frame the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIo3BkXnTISg",
    "papermill": {
     "duration": 0.042689,
     "end_time": "2020-10-15T16:08:16.777892",
     "exception": false,
     "start_time": "2020-10-15T16:08:16.735203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- The given problem is related with time wasted in granting and revoking access to the employee within company.  For employee to access any resources he/she needs prior permission i.e. access of that resource. The access granting and revoking process is manual, handled by superviso. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money.\n",
    "\n",
    "- <b>Objective:</b> We have to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted.\n",
    "\n",
    "\n",
    "- <b>Data:</b> The data consists of real historical data collected from 2010 & 2011. Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees.\n",
    "\n",
    "Test dataset (10 columns): The test set for which predictions should be made.  Each row asks whether an employee having the listed characteristics should have access to the listed resource.\n",
    "\n",
    "Training dataset (10 columns): Each row has the ACTION (ground truth), RESOURCE, and information about the employee's role at the time of approval.\n",
    "Following are the features present in the training dataset:\n",
    "- ACTION: Target variable. ACTION is 1 if the resource was approved, 0 if the resource was not approved.\n",
    "- RESOURCE: An ID for each resource\n",
    "- MGR_ID: The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time\n",
    "- ROLE_ROLLUP_1: Company role grouping category id 1 (e.g. US Engineering)\n",
    "- ROLE_ROLLUP_2: Company role grouping category id 2 (e.g. US Retail)\n",
    "- ROLE_DEPTNAME: Company role department name (e.g. Retail)\n",
    "- ROLE_TITLE: Company role business title description (e.g. Senior Engineering Retail Manager)\n",
    "- ROLE_FAMILY_DESC: Company role family extended description (e.g. Retail Manager, Software Engineering)\n",
    "- ROLE_FAMILY: Company role family description (e.g. Retail Manager)\n",
    "- ROLE_CODE: Company role code; this code is unique to each role (e.g. Manager)\n",
    "\n",
    "All features has numerical values but all features are categorical features.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KnR-E6tTdK6"
   },
   "source": [
    "## Install Vectice and modules needed to connect to Google Cloud Storage(GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iWoITyrtEZm"
   },
   "source": [
    "Vectice provides a generic metadata layer that is potentially suitable for most data science workflows. For this tutorial we will use the sickit-learn library for modeling and track experiments directly through our Python SDK to illustrate how to fine-tune exactly what you would like to track: metrics, etc. The same mechanisms would apply to R, Java or even more generic REST APIs to track metadata from any programming language and library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T8eJr9xxDxn"
   },
   "source": [
    "Here is a link to the Python SDK Documentation, it's not final nor complete and it is updated as we go along. \n",
    "[Python SDK Documentation](https://storage.googleapis.com/sdk-documentation/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82jlca9kTfCx"
   },
   "outputs": [],
   "source": [
    "## Requirements\n",
    "!pip3 install -q fsspec\n",
    "!pip3 install -q gcsfs\n",
    "!pip3 install -q vectice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccts403STkVn"
   },
   "outputs": [],
   "source": [
    "!pip3 show vectice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVKjnCNzTnVp"
   },
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:16.872102Z",
     "iopub.status.busy": "2020-10-15T16:08:16.871134Z",
     "iopub.status.idle": "2020-10-15T16:08:17.939531Z",
     "shell.execute_reply": "2020-10-15T16:08:17.938528Z"
    },
    "id": "arVGjVeZTISj",
    "papermill": {
     "duration": 1.118314,
     "end_time": "2020-10-15T16:08:17.939678",
     "exception": false,
     "start_time": "2020-10-15T16:08:16.821364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vectice.models import JobType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gkth-sSZTuo2"
   },
   "source": [
    "## Get the training and test data from GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5FjqgyjT09K"
   },
   "outputs": [],
   "source": [
    "# Load your json key file to access GCS that was provided with your tutorial account. \n",
    "# The name should be something like GCS_key.json.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oApCUClIDLhA"
   },
   "outputs": [],
   "source": [
    "# Once your file is loaded set the credentials for GCS and load the file\n",
    "# in a pandas frame, double check the json file name you uploaded.\n",
    "\n",
    "### Complete with the name of the file your json key file to access GCS that was provided with your tutorial account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'Name of the GCS key access file you uploaded'\n",
    "\n",
    "# The original source dataset is already declared in the Vectice UI as \"amazon_employee_access_train.csv\",\n",
    "# and its connection to \"gs://vectice-examples-samples/Amazon_challenge/\" has been established.\n",
    "data = pd.read_csv('gs://vectice-examples-samples/Amazon_challenge/dataset.csv')\n",
    "# Run head to make sure the data was loaded properly\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:18.038159Z",
     "iopub.status.busy": "2020-10-15T16:08:18.036943Z",
     "iopub.status.idle": "2020-10-15T16:08:18.144186Z",
     "shell.execute_reply": "2020-10-15T16:08:18.143571Z"
    },
    "id": "H8OIGy2dTISl",
    "papermill": {
     "duration": 0.160527,
     "end_time": "2020-10-15T16:08:18.144317",
     "exception": false,
     "start_time": "2020-10-15T16:08:17.983790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xklmP-FXTISq",
    "papermill": {
     "duration": 0.055283,
     "end_time": "2020-10-15T16:08:18.245037",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.189754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-NKIjrWUCS0"
   },
   "source": [
    "Data exploration enables us to take a first look on the data, can enhance the overall understanding of the characteristics of the data domain and helps to detect correlation between the features, thereby allowing for the creation of more accurate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:18.349444Z",
     "iopub.status.busy": "2020-10-15T16:08:18.348286Z",
     "iopub.status.idle": "2020-10-15T16:08:18.353917Z",
     "shell.execute_reply": "2020-10-15T16:08:18.353247Z"
    },
    "id": "j31At0hlTISr",
    "papermill": {
     "duration": 0.063886,
     "end_time": "2020-10-15T16:08:18.354043",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.290157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:18.453482Z",
     "iopub.status.busy": "2020-10-15T16:08:18.452289Z",
     "iopub.status.idle": "2020-10-15T16:08:18.467495Z",
     "shell.execute_reply": "2020-10-15T16:08:18.466716Z"
    },
    "id": "Y0mrxzGWTISs",
    "papermill": {
     "duration": 0.068238,
     "end_time": "2020-10-15T16:08:18.467626",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.399388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQas3mmsTISt",
    "papermill": {
     "duration": 0.051153,
     "end_time": "2020-10-15T16:08:18.564995",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.513842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is no column with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:18.673017Z",
     "iopub.status.busy": "2020-10-15T16:08:18.671897Z",
     "iopub.status.idle": "2020-10-15T16:08:18.688578Z",
     "shell.execute_reply": "2020-10-15T16:08:18.687779Z"
    },
    "id": "NmC-oL2DTISu",
    "papermill": {
     "duration": 0.07767,
     "end_time": "2020-10-15T16:08:18.688714",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.611044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqJ4dAT3TISv",
    "papermill": {
     "duration": 0.0458,
     "end_time": "2020-10-15T16:08:18.781154",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.735354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- In dataset description it is mention that an employee can have only one manager at a time, then we can consider that the dataset contains information of maximum 4243 employees.\n",
    "- There are same number of unique values for ROLE_TITLE and ROLE_CODE. There is 1-to-1 mapping between these columns. So for our problem only one feature is sufficent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:18.885058Z",
     "iopub.status.busy": "2020-10-15T16:08:18.884258Z",
     "iopub.status.idle": "2020-10-15T16:08:19.059166Z",
     "shell.execute_reply": "2020-10-15T16:08:19.058398Z"
    },
    "id": "WmmQgxxqTISw",
    "papermill": {
     "duration": 0.227781,
     "end_time": "2020-10-15T16:08:19.059318",
     "exception": false,
     "start_time": "2020-10-15T16:08:18.831537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='ACTION', data=data_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40dN9NYNUc5m"
   },
   "outputs": [],
   "source": [
    "data['ACTION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOvXAqszTISx",
    "papermill": {
     "duration": 0.048468,
     "end_time": "2020-10-15T16:08:19.155982",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.107514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- We can see that we have an imbalanced dataset. There are very less records of not granting the access. some algorithms may learn just from the ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqlvAQvzUUDV"
   },
   "outputs": [],
   "source": [
    "## Manager ID and how much resources he has access to\n",
    "data['MGR_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jF2Iw9BTISy",
    "papermill": {
     "duration": 0.047035,
     "end_time": "2020-10-15T16:08:19.252507",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.205472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Lets find out top 15 Resources, Role department, Role family, Role codes for which most access is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:19.357927Z",
     "iopub.status.busy": "2020-10-15T16:08:19.356856Z",
     "iopub.status.idle": "2020-10-15T16:08:19.376182Z",
     "shell.execute_reply": "2020-10-15T16:08:19.375355Z"
    },
    "id": "bo5oEnFXTISz",
    "papermill": {
     "duration": 0.07575,
     "end_time": "2020-10-15T16:08:19.376315",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.300565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore_resources = data_explore[['RESOURCE', \"ACTION\"]].groupby(by='RESOURCE').count()\n",
    "data_explore_resources.sort_values('ACTION', ascending=False).head(n=15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:19.483038Z",
     "iopub.status.busy": "2020-10-15T16:08:19.481908Z",
     "iopub.status.idle": "2020-10-15T16:08:19.498392Z",
     "shell.execute_reply": "2020-10-15T16:08:19.497728Z"
    },
    "id": "rNMwE8DMTIS0",
    "papermill": {
     "duration": 0.073718,
     "end_time": "2020-10-15T16:08:19.498518",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.424800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore_role_dept = data_explore[['ROLE_DEPTNAME', \"ACTION\"]].groupby(by='ROLE_DEPTNAME').count()\n",
    "data_explore_role_dept.sort_values('ACTION', ascending=False).head(n=15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:19.606351Z",
     "iopub.status.busy": "2020-10-15T16:08:19.605555Z",
     "iopub.status.idle": "2020-10-15T16:08:19.622097Z",
     "shell.execute_reply": "2020-10-15T16:08:19.621236Z"
    },
    "id": "F2c1zQSlTIS1",
    "papermill": {
     "duration": 0.074037,
     "end_time": "2020-10-15T16:08:19.622232",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.548195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore_role_codes = data_explore[['ROLE_CODE', \"ACTION\"]].groupby(by='ROLE_CODE').count()\n",
    "data_explore_role_codes.sort_values('ACTION', ascending=False).head(n=15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:19.745722Z",
     "iopub.status.busy": "2020-10-15T16:08:19.744459Z",
     "iopub.status.idle": "2020-10-15T16:08:19.769136Z",
     "shell.execute_reply": "2020-10-15T16:08:19.770234Z"
    },
    "id": "XZk0Ts1mTIS1",
    "papermill": {
     "duration": 0.09194,
     "end_time": "2020-10-15T16:08:19.770470",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.678530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_explore_role_family = data_explore[['ROLE_FAMILY', \"ACTION\"]].groupby(by='ROLE_FAMILY').count()\n",
    "data_explore_role_family.sort_values('ACTION', ascending=False).head(n=15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvpiYy2qUybn"
   },
   "outputs": [],
   "source": [
    "## We use data.describe() to only take numerical columns ,and avoid non numerical ones, in order to plot them\n",
    "for i in data.describe().columns:\n",
    "  sns.distplot(data[i].dropna())\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKL7m-3hskLx"
   },
   "source": [
    "### Correlation\n",
    "\n",
    "If we have a big correlation, we have a problem of multicolinearity. That means that there are some features that depend of other features, so we should reduce the dimentionality of our data (if A depends of B, we should either find a way to aggregate or combine the two features and turn it into one variable or drop one of the variables that are too highly correlated with another) and that can be adressed using Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:19.888530Z",
     "iopub.status.busy": "2020-10-15T16:08:19.887676Z",
     "iopub.status.idle": "2020-10-15T16:08:20.513889Z",
     "shell.execute_reply": "2020-10-15T16:08:20.513220Z"
    },
    "id": "6Vz3uKF0TIS2",
    "papermill": {
     "duration": 0.684604,
     "end_time": "2020-10-15T16:08:20.514017",
     "exception": false,
     "start_time": "2020-10-15T16:08:19.829413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## If we have a big correlation, we have a problem of multicolinearity that can be adressed using PCA\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(data.corr(),annot=True,cmap='viridis',linewidth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:20.630370Z",
     "iopub.status.busy": "2020-10-15T16:08:20.629477Z",
     "iopub.status.idle": "2020-10-15T16:08:20.634298Z",
     "shell.execute_reply": "2020-10-15T16:08:20.633513Z"
    },
    "id": "JD2HZ9E_TIS3",
    "papermill": {
     "duration": 0.066424,
     "end_time": "2020-10-15T16:08:20.634454",
     "exception": false,
     "start_time": "2020-10-15T16:08:20.568030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = data_explore.corr()\n",
    "corr_matrix['ACTION'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhWY7tLrTIS4",
    "papermill": {
     "duration": 0.060208,
     "end_time": "2020-10-15T16:08:20.758519",
     "exception": false,
     "start_time": "2020-10-15T16:08:20.698311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- There is no attribute to which target variable is strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwHD-UTNVNkj"
   },
   "source": [
    "## Connect to your Vectice workspace and your Vectice project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OExV-YrztyIG"
   },
   "source": [
    "Here we are going to need an API token and a project token. An API token is used to secure requests between your existing tools and Vectice. You can create and manage those at the API Tokens tab in your workspace, and they impersonate you and your rights per workspace, so we strongly recommend you to avoid sharing them. A project token is used to target the project you're working on in the UI and can found (after creating a project) in the Project settings page, and anyone working on the project can see it and copy/paste it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGuJnWMKt2RD"
   },
   "outputs": [],
   "source": [
    "# In order to use Vectice SDK, let's set up the configurations first.\n",
    "# The Vectice API key below can be generated from the UI.\n",
    "# For better security, the settings can also be put into a dedicated file called `.vectice` or `.env`.\n",
    "## Make sure that you're using the right endpoint (hint: be-beta.vectice.com)\n",
    "os.environ['VECTICE_API_ENDPOINT']= \"\"\n",
    "os.environ['VECTICE_API_TOKEN'] = \"\"\n",
    "\n",
    "## Create a Vetice instance to connect to your project using your project token\n",
    "## Hint: Do not forget to import vectice (from vectice import Vectice)\n",
    "vectice = \"\"\n",
    "\n",
    "print(vectice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lzZ7PWNYt3qV"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the syntax\n",
    "os.environ['VECTICE_API_ENDPOINT']= \"be-beta.vectice.com\"\n",
    "\n",
    "##Complete with your Vectice API token\n",
    "os.environ['VECTICE_API_TOKEN'] = \"\"\n",
    "from vectice import Vectice\n",
    "## Complete with your project token\n",
    "vectice = Vectice(project_token=\"\")\n",
    "print(vectice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V487vfI9TIS4",
    "papermill": {
     "duration": 0.054109,
     "end_time": "2020-10-15T16:08:20.866855",
     "exception": false,
     "start_time": "2020-10-15T16:08:20.812746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing alludes to the method of cleaning and arranging the crude data to make it appropriate for building and preparing AI models. Data preprocessing is a procedure that changes crude data into an instructive and lucid arrangement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scl3PYmJtfuQ"
   },
   "source": [
    "Our training data contains categorical data (information that is divided into groups, it could be numbers but without mathematical meaning like IDs). Thus, we should encode it or use algorithms that can deal with categorical data like CatBoost. If we don't preprocess this categorical data, some algorithms may learn false information. For example, an ID of 1000 would be considered as much greater than an ID of 1 which is not necessarily true. Thus we can use OneHotEncoder to before using algorithms on categorical data. One-hot encoding in machine learning is the conversion of categorical information into a format that may be fed into machine learning algorithms to improve prediction accuracy. It is a common method for dealing with categorical data in machine learning. Categorical variables must be changed in the pre-processing section since machine learning models require numeric input variables. Nominal or ordinal data can be found in categorical data. This approach creates a new column for each unique value in the original category column. The zeros and ones are subsequently put in these dummy variables (1 meaning TRUE, 0 meaning FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:20.982290Z",
     "iopub.status.busy": "2020-10-15T16:08:20.981324Z",
     "iopub.status.idle": "2020-10-15T16:08:21.332009Z",
     "shell.execute_reply": "2020-10-15T16:08:21.331208Z"
    },
    "id": "3I9Y_KOVTIS5",
    "papermill": {
     "duration": 0.411914,
     "end_time": "2020-10-15T16:08:21.332141",
     "exception": false,
     "start_time": "2020-10-15T16:08:20.920227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:21.454139Z",
     "iopub.status.busy": "2020-10-15T16:08:21.453232Z",
     "iopub.status.idle": "2020-10-15T16:08:21.463067Z",
     "shell.execute_reply": "2020-10-15T16:08:21.462402Z"
    },
    "id": "oBoMqRUVTIS5",
    "papermill": {
     "duration": 0.074693,
     "end_time": "2020-10-15T16:08:21.463196",
     "exception": false,
     "start_time": "2020-10-15T16:08:21.388503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns=['ACTION'], axis=1).copy()\n",
    "y = data['ACTION'].copy()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:21.580097Z",
     "iopub.status.busy": "2020-10-15T16:08:21.578793Z",
     "iopub.status.idle": "2020-10-15T16:08:21.583477Z",
     "shell.execute_reply": "2020-10-15T16:08:21.582836Z"
    },
    "id": "Gvn3YgvfTIS6",
    "papermill": {
     "duration": 0.066358,
     "end_time": "2020-10-15T16:08:21.583608",
     "exception": false,
     "start_time": "2020-10-15T16:08:21.517250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_attrs = list(X.columns)\n",
    "cat_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjTp6bCJuUdm"
   },
   "source": [
    "Create a dataset containing your dataset to use it as input for your splitting data job. That can be done through the UI by going to your project, clicking on datasets and then click on add (you should add a connection to be able to create a dataset)\n",
    "\n",
    "Create a dataset version based on the dataset you created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peAwDCczuHxs"
   },
   "outputs": [],
   "source": [
    "input_ds_version = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yd5YigZGuOu7"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the syntax\n",
    "# Use auto-versioning here\n",
    "input_ds_version =  vectice.create_dataset_version().with_parent_name(\"Dataset name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwx1HlUZvP2R"
   },
   "source": [
    "The following code splits the dataset to train and test sets. Please complete itwith creating PREPARATION job run, start it and then declare train_set and test_set as dataset versions (after creating the datasets in the UI) in order to be able to use them as inputs for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz7vWKhiunsm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "  strat_train_set = data.iloc[train_index]\n",
    "  strat_test_set = data.iloc[test_index]\n",
    "\n",
    "X_train = strat_train_set.drop('ACTION', axis=1)\n",
    "y_train = strat_train_set['ACTION'].copy()\n",
    "X_test = strat_test_set.drop('ACTION', axis=1)\n",
    "y_test = strat_test_set['ACTION'].copy()\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "train_set = X_train.join(y_train)\n",
    "test_set = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hrmRlyIhtzOr"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the answer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "input_ds_version = input_ds_version\n",
    "\n",
    "# Start a Vectice run. The job type should be PREPARATION in this case.\n",
    "\n",
    "vectice.create_run(\"jobSplitData\", JobType.PREPARATION)\n",
    "with vectice.start_run(inputs=[input_ds_version, input_code]) as run:\n",
    "\n",
    "  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "  for train_index, test_index in split.split(X, y):\n",
    "      strat_train_set = data.iloc[train_index]\n",
    "      strat_test_set = data.iloc[test_index]\n",
    "\n",
    "  X_train = strat_train_set.drop('ACTION', axis=1)\n",
    "  y_train = strat_train_set['ACTION'].copy()\n",
    "  X_test = strat_test_set.drop('ACTION', axis=1)\n",
    "  y_test = strat_test_set['ACTION'].copy()\n",
    "  X_train.shape, X_test.shape\n",
    "\n",
    "  train_set = X_train.join(y_train)\n",
    "  test_set = X_test.join(y_test)\n",
    "# We commented out the code to persist the training and testing test in GCS,\n",
    "# because we already generated it for you, but feel free to uncomment it and execute it.\n",
    "# The key you were provided for this tutorial may not have write permissions to GCS.\n",
    "# Let us know if you want to be able to write files as well and we can issue you a different key.\n",
    "\n",
    "  #train_set.to_csv (r'gs://vectice-examples-samples/Amazon_challenge/training_data.csv', index = False, header = True)\n",
    "  #test_set.to_csv (r'gs://vectice-examples-samples/Amazon_challenge/testing_data.csv', index = False, header = True)\n",
    "\n",
    "  # Don't forget to create the datasets in the UI before creating train_ds_version and test_ds_version \n",
    "  train_ds_version = vectice.create_dataset_version().with_parent_name(\"Training dataset name\")\n",
    "  test_ds_version = vectice.create_dataset_version().with_parent_name(\"Testing dataset name\")\n",
    "  \n",
    "  run.add_outputs([train_ds_version,test_ds_version])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zywmsFo5tmRt"
   },
   "source": [
    "We create a pipeline with the OneHotEncoder for algorithms that doesn't support categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:21.887013Z",
     "iopub.status.busy": "2020-10-15T16:08:21.886124Z",
     "iopub.status.idle": "2020-10-15T16:08:21.989771Z",
     "shell.execute_reply": "2020-10-15T16:08:21.989065Z"
    },
    "id": "BBweUu04TIS7",
    "papermill": {
     "duration": 0.175769,
     "end_time": "2020-10-15T16:08:21.989899",
     "exception": false,
     "start_time": "2020-10-15T16:08:21.814130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                         ('cat_enc', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "pre_process = ColumnTransformer([('cat_process', cat_pipeline, cat_attrs)], remainder='passthrough')\n",
    "\n",
    "X_train_transformed = pre_process.fit_transform(X_train)\n",
    "X_test_transformed = pre_process.transform(X_test)\n",
    "X_train_transformed.shape, X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYCoaHjiTIS8",
    "papermill": {
     "duration": 0.056567,
     "end_time": "2020-10-15T16:08:22.105733",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.049166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Since we will be using CatBoost Classifier. For CatBoost model, there is no need of encoding categorical model. Hence we will be creating a separate preprocessing pipeline for CatBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:22.231547Z",
     "iopub.status.busy": "2020-10-15T16:08:22.230453Z",
     "iopub.status.idle": "2020-10-15T16:08:22.262458Z",
     "shell.execute_reply": "2020-10-15T16:08:22.261775Z"
    },
    "id": "EgmwvQCITIS8",
    "papermill": {
     "duration": 0.100032,
     "end_time": "2020-10-15T16:08:22.262592",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.162560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_boost_pre_process = ColumnTransformer([('imputer', SimpleImputer(strategy='most_frequent'), cat_attrs)], remainder='passthrough')\n",
    "\n",
    "X_cb_train_transformed = cat_boost_pre_process.fit_transform(X_train)\n",
    "X_cb_test_transformed = cat_boost_pre_process.transform(X_test)\n",
    "X_cb_train_transformed.shape, X_cb_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:22.413099Z",
     "iopub.status.busy": "2020-10-15T16:08:22.412209Z",
     "iopub.status.idle": "2020-10-15T16:08:22.436604Z",
     "shell.execute_reply": "2020-10-15T16:08:22.435864Z"
    },
    "id": "LeqMeHWATIS9",
    "papermill": {
     "duration": 0.108777,
     "end_time": "2020-10-15T16:08:22.436774",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.327997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = list(pre_process.transformers_[0][1]['cat_enc'].get_feature_names(cat_attrs))\n",
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIPqz8-YVp1_"
   },
   "source": [
    "## Get different user versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SWJz9CTVwdT"
   },
   "source": [
    "Generate a random user version by calling get_random_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt5tbMmdVzkX"
   },
   "outputs": [],
   "source": [
    "# Let's generate some unique names for our following modeling experiments\n",
    "import random\n",
    "import string\n",
    "def get_random_string(length):\n",
    "    return \"\".join(random.choice(string.ascii_letters) for i in range(length))\n",
    "\n",
    "rdm_str = get_random_string(5)\n",
    "print(\"Generated random string to make job names unique:\", rdm_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfFabR0LTIS9",
    "papermill": {
     "duration": 0.063239,
     "end_time": "2020-10-15T16:08:22.561166",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.497927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZkU-11qTIS9",
    "papermill": {
     "duration": 0.062076,
     "end_time": "2020-10-15T16:08:22.686030",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.623954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Evaluation metric for this competition is ROC AUC Score. Roc_auc_score : ROC curve is a graph that shows the performance of a classification model at all possible thresholds( threshold is a particular value beyond which you say a point belongs to a particular class). AUC measures how well a model is able to distinguish between classes. The curve is plotted between two parameters :\n",
    "    * TRUE POSITIVE RATE\n",
    "    * FALSE POSTIVIE RATE\n",
    "- Since we have imbalance dataset, we will use Matthews correlation coefficient (MCC) as another evaluation metric. \n",
    "- Value of MCC is lies between -1 to +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "    - ${MCC} = \\frac{(TP + TN) * (FP + FN)}{\\sqrt{( (TP +FP) * (TP + FN) * (TN + FP) * (TN + FN))}}$\n",
    "\n",
    "     * True Positive (TP): The value of correct predictions of positives out of actual positive cases. Ex : Predict a well person as not sick\n",
    "   * False Positive (FP): The value of incorrect positive predictions. The number of negatives falsly pridected as positives. Ex : Predict a sick person as not sick\n",
    "  * True Negative (TN): The value of correct predictions of negatives out of actual negative cases. Ex : Predict a sick person as sick\n",
    "  * False Negative (FN): The value of incorrect negative predictions. This value represents the number of positives which gets falsely. Predict a well person as sick\n",
    "    \n",
    "- MCC value will be high only if model has high accuracy on predictions of negative data instances as well as of positive data instances\n",
    "- We will be selecting the best model with highest ROC AUC Score\n",
    "- Here we use cross-validation which is a statistical method used to estimate the performance (or accuracy) of machine learning models and protect them from overfitting ((the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably)). The main idea of cross-validation is to make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:22.828098Z",
     "iopub.status.busy": "2020-10-15T16:08:22.827276Z",
     "iopub.status.idle": "2020-10-15T16:08:22.831325Z",
     "shell.execute_reply": "2020-10-15T16:08:22.830571Z"
    },
    "id": "H6RaYtvwTIS9",
    "papermill": {
     "duration": 0.068427,
     "end_time": "2020-10-15T16:08:22.831518",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.763091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:23.063376Z",
     "iopub.status.busy": "2020-10-15T16:08:23.062573Z",
     "iopub.status.idle": "2020-10-15T16:08:23.066395Z",
     "shell.execute_reply": "2020-10-15T16:08:23.065787Z"
    },
    "id": "JA65CUmOTIS-",
    "papermill": {
     "duration": 0.176391,
     "end_time": "2020-10-15T16:08:23.066527",
     "exception": false,
     "start_time": "2020-10-15T16:08:22.890136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, make_scorer, roc_auc_score, roc_curve\n",
    "Matthew = make_scorer(matthews_corrcoef)\n",
    "\n",
    "results = []\n",
    "\n",
    "def plot_custom_roc_curve(clf_name, y_true, y_scores):\n",
    "    auc_score = np.round(roc_auc_score(y_true, y_scores), 3)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=clf_name+\" (AUC Score: {})\".format(str(auc_score)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel(\"FPR\", fontsize=16)\n",
    "    plt.ylabel(\"TPR\", fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def performance_measures(model, X_tr=X_train_transformed, y_tr=y_train, X_ts=X_test_transformed, y_ts=y_test,\n",
    "                         store_results=True):\n",
    "    train_mcc = cross_val_score(model, X_tr, y_tr, scoring=Matthew, cv=kf, n_jobs=-1)\n",
    "    test_mcc = cross_val_score(model, X_ts, y_ts, scoring=Matthew, cv=kf, n_jobs=-1)\n",
    "    print(\"Mean Train MCC: {}\\nMean Test MCC: {}\".format(train_mcc.mean(), test_mcc.mean()))\n",
    "\n",
    "    \n",
    "    train_roc_auc = cross_val_score(model, X_tr, y_tr, scoring='roc_auc', cv=kf, n_jobs=-1)\n",
    "    test_roc_auc = cross_val_score(model, X_ts, y_ts, scoring='roc_auc', cv=kf, n_jobs=-1)\n",
    "    print(\"Mean Train ROC AUC Score: {}\\nMean Test ROC AUC Score: {}\".format(train_roc_auc.mean(), test_roc_auc.mean()))\n",
    "    return train_mcc.mean(), test_mcc.mean(), train_roc_auc.mean(), test_roc_auc.mean()\n",
    "    \n",
    "    if store_results:\n",
    "        results.append([model.__class__.__name__, np.round(np.mean(train_roc_auc), 3), np.round(np.mean(test_roc_auc), 3), np.round(np.mean(train_mcc), 3), np.round(np.mean(test_mcc), 3)])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:23.195123Z",
     "iopub.status.busy": "2020-10-15T16:08:23.193982Z",
     "iopub.status.idle": "2020-10-15T16:08:23.197578Z",
     "shell.execute_reply": "2020-10-15T16:08:23.196821Z"
    },
    "id": "CIbGOO9oTIS-",
    "papermill": {
     "duration": 0.072418,
     "end_time": "2020-10-15T16:08:23.197707",
     "exception": false,
     "start_time": "2020-10-15T16:08:23.125289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_columns, importance_values, top_n_features=10):\n",
    "    feature_imp = [ col for col in zip(feature_columns, importance_values)]\n",
    "    feature_imp.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    if top_n_features:\n",
    "        imp = pd.DataFrame(feature_imp[0:top_n_features], columns=['feature', 'importance'])\n",
    "    else:\n",
    "        imp = pd.DataFrame(feature_imp, columns=['feature', 'importance'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(y='feature', x='importance', data=imp, orient='h')\n",
    "    plt.title('Most Important Features', fontsize=16)\n",
    "    plt.ylabel(\"Feature\", fontsize=16)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzIKy37qu8Ha"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression is used to calculate the probability of a binary event occurring. For example, predicting if a credit card transaction is fraudulent or not fraudulent or predicting if an incoming email is spam or not spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcVP2ofKvC-y"
   },
   "source": [
    "The following code creates a Logistic regression model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2cfJEC8vFmG"
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a run\n",
    "# Start the run\n",
    "\n",
    "logistic_reg = LogisticRegression(solver='liblinear', C=1, penalty='l2', max_iter=1000, random_state=42, n_jobs=-1)\n",
    "logistic_reg.fit(X_train_transformed, y_train)\n",
    "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(logistic_reg)\n",
    "\n",
    "\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nKMOepa4vJiS"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the answer\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "\n",
    "vectice.create_run(job_name = \"Train with Logistic regression\", job_type=JobType.TRAINING)\n",
    "## We use with here to catch errors and put the run's statut as failed in the UI in case we encounter an error, else it ends the successful runs\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\n",
    "  solver='liblinear'\n",
    "  C=1\n",
    "  penalty='l2'\n",
    "  max_iter=1000\n",
    "  random_state=42\n",
    "  n_jobs=-1\n",
    "  logistic_reg = LogisticRegression(solver=solver, C=C, penalty=penalty, max_iter=max_iter, random_state=random_state, n_jobs=n_jobs)\n",
    "  logistic_reg.fit(X_train_transformed, y_train)\n",
    "  train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(logistic_reg)\n",
    "\n",
    "  metrics = [(\"Train_mcc\",train_mcc),(\"test_mcc\",test_mcc), (\"train_roc_auc\", train_roc_auc), ('test_roc_auc', test_roc_auc)]\n",
    "  properties = [(\"solver\",solver), (C, C), (\"penalty\", penalty), (\"max_iter\", max_iter), (\"random_state\", random_state), (\"n_jobs\", n_jobs)]\n",
    "  model_version = vectice.create_model_version().with_parent_name(\"Logistic Regression\").with_algorithm(\"Classification : Logistic Regression\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\n",
    "  run.add_outputs([model_version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:23.747393Z",
     "iopub.status.busy": "2020-10-15T16:08:23.746414Z",
     "iopub.status.idle": "2020-10-15T16:08:24.037361Z",
     "shell.execute_reply": "2020-10-15T16:08:24.037969Z"
    },
    "id": "uxPqjduGTIS_",
    "papermill": {
     "duration": 0.373779,
     "end_time": "2020-10-15T16:08:24.038156",
     "exception": false,
     "start_time": "2020-10-15T16:08:23.664377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(feature_columns, logistic_reg.coef_[0], top_n_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9icQG8w7vAUc"
   },
   "source": [
    "### Random Forest\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. A decision tree is a type of flowchart that shows a clear pathway to a decision. A decision tree starts at a single point (or ‘node’) which then branches (or ‘splits’) in two or more directions going all the way to the leaves that give the decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFTY26Zfvehp"
   },
   "source": [
    "The following code creates a Random Forest model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcCjSg53vjFi"
   },
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a run\n",
    "# Start the run\n",
    "forest_clf = RandomForestClassifier(n_estimators=300, max_depth=16, random_state=42,n_jobs=-1)\n",
    "forest_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(forest_clf)\n",
    "\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:27.787024Z",
     "iopub.status.busy": "2020-10-15T16:08:27.786201Z",
     "iopub.status.idle": "2020-10-15T16:08:30.387017Z",
     "shell.execute_reply": "2020-10-15T16:08:30.387601Z"
    },
    "id": "7lISF6y2TITA",
    "papermill": {
     "duration": 2.66877,
     "end_time": "2020-10-15T16:08:30.387773",
     "exception": false,
     "start_time": "2020-10-15T16:08:27.719003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "\n",
    "vectice.create_run(job_name = \"Training Random Forest\", job_type=JobType.TRAINING)\n",
    "## We use with here to catch errors and put the run's statut as failed in the UI in case we encounter an error, else it ends the successful runs\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\n",
    "  n_estimators=300\n",
    "  max_depth=16\n",
    "  random_state=42\n",
    "  n_jobs=-1\n",
    "\n",
    "  forest_clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state,n_jobs=n_jobs)\n",
    "  forest_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "  train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(forest_clf)\n",
    "\n",
    "  metrics = [(\"Train_mcc\",train_mcc),(\"test_mcc\",test_mcc), (\"train_roc_auc\", train_roc_auc), ('test_roc_auc', test_roc_auc)]\n",
    "  properties = [(\"n_estimators\",n_estimators), (\"max_depth\", max_depth), (\"random_state\", random_state), (\"n_jobs\", n_jobs)]\n",
    "\n",
    "  model_version1 = vectice.create_model_version().with_parent_name(\"Random Forest\").with_algorithm(\"Random Forest\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\n",
    "  run.add_outputs([model_version1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:08:30.518839Z",
     "iopub.status.busy": "2020-10-15T16:08:30.517520Z",
     "iopub.status.idle": "2020-10-15T16:08:31.038419Z",
     "shell.execute_reply": "2020-10-15T16:08:31.037634Z"
    },
    "id": "knuEAe5YTITA",
    "papermill": {
     "duration": 0.588144,
     "end_time": "2020-10-15T16:08:31.038549",
     "exception": false,
     "start_time": "2020-10-15T16:08:30.450405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(feature_columns, forest_clf.feature_importances_, top_n_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsk--891vDc_"
   },
   "source": [
    "###XGBoost\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKlnX5fU-hif"
   },
   "source": [
    "The following code creates a XGBoost model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSrrXIWy-kFG"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Create a run\n",
    "# Start the run\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=300, max_depth=16, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "xgb_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(xgb_clf)\n",
    "\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\n",
    "## End the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-10-15T16:09:41.221193Z",
     "iopub.status.busy": "2020-10-15T16:09:41.220253Z",
     "iopub.status.idle": "2020-10-15T16:10:04.232901Z",
     "shell.execute_reply": "2020-10-15T16:10:04.233658Z"
    },
    "id": "2rA5PNaWTITB",
    "papermill": {
     "duration": 23.082833,
     "end_time": "2020-10-15T16:10:04.233832",
     "exception": false,
     "start_time": "2020-10-15T16:09:41.150999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the answer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "\n",
    "vectice.create_run(job_name = \"Training with XGBClassifier\", job_type=JobType.TRAINING)\n",
    "## We use with here to catch errors and put the run's statut as failed in the UI in case we encounter an error, else it ends the successful runs\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\n",
    "  n_estimators=300\n",
    "  max_depth=16\n",
    "  learning_rate=0.1\n",
    "  random_state=42\n",
    "  n_jobs=-1\n",
    "\n",
    "  xgb_clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=random_state, n_jobs=n_jobs)\n",
    "  xgb_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "  train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(xgb_clf)\n",
    "  properties = [(\"n_estimators\",n_estimators), (\"max_depth\", max_depth), (\"learning_rate\", learning_rate), (\"random_state\", random_state), (\"n_jobs\", n_jobs)]\n",
    "\n",
    "  metrics = [(\"Train_mcc\",train_mcc),(\"test_mcc\",test_mcc), (\"train_roc_auc\", train_roc_auc), ('test_roc_auc', test_roc_auc)]\n",
    "\n",
    "  model_version2 = vectice.create_model_version().with_parent_name(\"XGBoost\").with_algorithm(\"XGBoost Classification\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\n",
    "\n",
    "  run.add_outputs([model_version2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:10:04.368807Z",
     "iopub.status.busy": "2020-10-15T16:10:04.367966Z",
     "iopub.status.idle": "2020-10-15T16:10:04.895797Z",
     "shell.execute_reply": "2020-10-15T16:10:04.895078Z"
    },
    "id": "_LIvwO7DTITC",
    "papermill": {
     "duration": 0.597026,
     "end_time": "2020-10-15T16:10:04.895929",
     "exception": false,
     "start_time": "2020-10-15T16:10:04.298903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(feature_columns, xgb_clf.feature_importances_, top_n_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLcFQi0ReTIp"
   },
   "source": [
    "### Catboost\n",
    "\n",
    "Catboost is a boosted decision tree machine learning algorithm. It works in the same way as other gradient boosted algorithms such as XGBoost but provides support out of the box for categorical variables, has a higher level of accuracy without tuning parameters and also offers GPU support to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vg-e6bg_eQfi"
   },
   "outputs": [],
   "source": [
    "!pip install -q catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v9dgq7Y_xjF"
   },
   "source": [
    "The following code creates a Catboost model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_pqFOq8_0X-"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Create a run\n",
    "#Start the run\n",
    "\n",
    "catboost_clf = CatBoostClassifier(loss_function='Logloss', iterations=500, depth=6, l2_leaf_reg=1, \n",
    "                                  cat_features=list(range(X_cb_train_transformed.shape[1])), \n",
    "                                  eval_metric='AUC', random_state=42, verbose=0)\n",
    "catboost_clf.fit(X_cb_train_transformed, y_train)\n",
    "\n",
    "\n",
    "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(catboost_clf, X_tr=X_cb_train_transformed, X_ts=X_cb_test_transformed)\n",
    "\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-10-15T16:13:00.960791Z",
     "iopub.status.busy": "2020-10-15T16:13:00.959979Z",
     "iopub.status.idle": "2020-10-15T16:13:24.132024Z",
     "shell.execute_reply": "2020-10-15T16:13:24.130542Z"
    },
    "id": "o1KYFAghTITC",
    "papermill": {
     "duration": 23.246009,
     "end_time": "2020-10-15T16:13:24.132197",
     "exception": false,
     "start_time": "2020-10-15T16:13:00.886188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the answer\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "\n",
    "vectice.create_run(job_name = \"Training with CatBoost\", job_type=JobType.TRAINING)\n",
    "## We use with here to catch errors and put the run's statut as failed in the UI in case we encounter an error, else it ends the successful runs\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\n",
    "  loss_function='Logloss'\n",
    "  iterations=500\n",
    "  depth=6\n",
    "  eval_metric='AUC'\n",
    "  l2_leaf_reg=1\n",
    "  random_state=42\n",
    "  verbose=0\n",
    "  cat_features=list(range(X_cb_train_transformed.shape[1]))\n",
    "  catboost_clf = CatBoostClassifier(loss_function=loss_function, iterations=iterations, depth=depth, l2_leaf_reg=l2_leaf_reg, \n",
    "                                    cat_features=cat_features, \n",
    "                                    eval_metric=eval_metric, random_state=random_state, verbose=verbose)\n",
    "  catboost_clf.fit(X_cb_train_transformed, y_train)\n",
    "\n",
    "  train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(catboost_clf, X_tr=X_cb_train_transformed, X_ts=X_cb_test_transformed)\n",
    "\n",
    "  metrics = [(\"Train_mcc\",train_mcc),(\"test_mcc\",test_mcc), (\"train_roc_auc\", train_roc_auc), ('test_roc_auc', test_roc_auc)]\n",
    "  properties = [(\"loss_function\",loss_function), (\"iterations\", iterations), (\"categorical features\",list(range(X_cb_train_transformed.shape[1]))),\n",
    "                (\"verbose\",verbose), (\"depth\", depth), (\"eval_metric\", eval_metric), (\"l2_leaf_reg\", l2_leaf_reg), (\"random_state\", random_state)]\n",
    "  model_version3 = vectice.create_model_version().with_parent_name(\"CatBoost\").with_algorithm(\"CatBoost Classification\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\n",
    "  run.add_outputs([model_version3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T16:18:50.007263Z",
     "iopub.status.busy": "2020-10-15T16:18:50.006292Z",
     "iopub.status.idle": "2020-10-15T16:18:50.217503Z",
     "shell.execute_reply": "2020-10-15T16:18:50.218052Z"
    },
    "id": "SadAMVnlTITD",
    "papermill": {
     "duration": 0.289119,
     "end_time": "2020-10-15T16:18:50.218217",
     "exception": false,
     "start_time": "2020-10-15T16:18:49.929098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(feature_columns, catboost_clf.feature_importances_, top_n_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmF7VU7VvVTJ"
   },
   "source": [
    "###Voting classifier\n",
    "\n",
    "A voting classifier is a classification method that employs multiple classifiers to make predictions. It is very applicable in situations when a data scientist or machine learning engineer is confused about which classification method to use. Therefore, using the predictions from multiple classifiers, the voting classifier makes predictions based on the most frequent one. It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4cXNj-xA0rU"
   },
   "source": [
    "The following code creates a Voting classifier model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XziYte64A4-e"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#create a run\n",
    "# Start the run\n",
    "\n",
    "logistic_reg_pipeline = Pipeline([('pre_process', pre_process), ('logistic_reg', logistic_reg)])\n",
    "forest_clf_pipeline = Pipeline([('pre_process', pre_process), ('forest_clf', forest_clf)])\n",
    "xgb_clf_pipeline = Pipeline([('pre_process', pre_process), ('xgb_clf', xgb_clf)])\n",
    "catboost_clf_pipeline = Pipeline([('pre_process', cat_boost_pre_process), ('catboost_clf', catboost_clf)])\n",
    "\n",
    "named_estimators = [('logistic_reg', logistic_reg_pipeline), ('forest_clf', forest_clf_pipeline), \n",
    "                    ('xgb_clf', xgb_clf_pipeline), ('catboost_clf', catboost_clf_pipeline)]\n",
    "\n",
    "voting_reg = VotingClassifier(estimators=named_estimators, voting='soft', n_jobs=-1)\n",
    "voting_reg.fit(X_train, y_train)\n",
    "\n",
    "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(voting_reg, X_tr=X_train, X_ts=X_test)\n",
    "\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-10-15T16:18:50.522837Z",
     "iopub.status.busy": "2020-10-15T16:18:50.522010Z",
     "iopub.status.idle": "2020-10-15T16:19:29.968873Z",
     "shell.execute_reply": "2020-10-15T16:19:29.969639Z"
    },
    "id": "jvw60OwkTITD",
    "papermill": {
     "duration": 39.5247,
     "end_time": "2020-10-15T16:19:29.969820",
     "exception": false,
     "start_time": "2020-10-15T16:18:50.445120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Double click to show the answer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\n",
    "script_relative_path=\"Notebooks/Vanilla/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\n",
    "\n",
    "\n",
    "vectice.create_run(job_name = \"Training with Voting Classifier\", job_type=JobType.TRAINING)\n",
    "## We use with here to catch errors and put the run's statut as failed in the UI in case we encounter an error, else it ends the successful runs\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\n",
    "  logistic_reg_pipeline = Pipeline([('pre_process', pre_process), ('logistic_reg', logistic_reg)])\n",
    "  forest_clf_pipeline = Pipeline([('pre_process', pre_process), ('forest_clf', forest_clf)])\n",
    "  xgb_clf_pipeline = Pipeline([('pre_process', pre_process), ('xgb_clf', xgb_clf)])\n",
    "  catboost_clf_pipeline = Pipeline([('pre_process', cat_boost_pre_process), ('catboost_clf', catboost_clf)])\n",
    "\n",
    "  voting='soft'\n",
    "  n_jobs=-1\n",
    "\n",
    "  named_estimators = [('logistic_reg', logistic_reg_pipeline), ('forest_clf', forest_clf_pipeline), \n",
    "                      ('xgb_clf', xgb_clf_pipeline), ('catboost_clf', catboost_clf_pipeline)]\n",
    "\n",
    "  voting_reg = VotingClassifier(estimators=named_estimators, voting=voting, n_jobs=n_jobs)\n",
    "  voting_reg.fit(X_train, y_train)\n",
    "  train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(voting_reg, X_tr=X_train, X_ts=X_test)\n",
    "\n",
    "  metrics = [(\"Train_mcc\",train_mcc),(\"test_mcc\",test_mcc), (\"train_roc_auc\", train_roc_auc), ('test_roc_auc', test_roc_auc)]\n",
    "  properties = [('voting', voting), ('n_jobs', n_jobs)]\n",
    "\n",
    "  model_version4 = vectice.create_model_version().with_parent_name(\"VotingClassifier\").with_algorithm(\"Voting Classifier\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\n",
    "\n",
    "  run.add_outputs([model_version4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NXG9HPhTITE",
    "papermill": {
     "duration": 0.07176,
     "end_time": "2020-10-15T16:26:58.887186",
     "exception": false,
     "start_time": "2020-10-15T16:26:58.815426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSqi4tuZRA07"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plot_custom_roc_curve('Logistic Regression', y_test, logistic_reg.decision_function(X_test_transformed))\n",
    "plot_custom_roc_curve('Random Forest', y_test, forest_clf.predict_proba(X_test_transformed)[:,1])\n",
    "plot_custom_roc_curve('XGBoost', y_test, xgb_clf.predict_proba(X_test_transformed)[:,1])\n",
    "plot_custom_roc_curve('CatBoost', y_test, catboost_clf.predict_proba(X_cb_test_transformed)[:,1])\n",
    "plot_custom_roc_curve('Soft Voting', y_test, voting_reg.predict_proba(X_test)[:,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mVKjnCNzTnVp",
    "xklmP-FXTISq",
    "RKL7m-3hskLx"
   ],
   "include_colab_link": true,
   "name": "Amazon_employee_access_challenge_users.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 1154.318387,
   "end_time": "2020-10-15T16:27:25.821888",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-15T16:08:11.503501",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
